{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfidf_svc.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fk_4ZPIbDX_",
        "outputId": "8430bc70-98c2-43fb-84d6-d285155bec2e"
      },
      "source": [
        "!pip install CatBoost\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from catboost import CatBoost\n",
        "from catboost import Pool\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: CatBoost in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from CatBoost) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from CatBoost) (1.15.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from CatBoost) (1.1.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from CatBoost) (0.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from CatBoost) (3.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from CatBoost) (4.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from CatBoost) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->CatBoost) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->CatBoost) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->CatBoost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->CatBoost) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->CatBoost) (2.4.7)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->CatBoost) (1.3.3)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UUCYpM7A7EB"
      },
      "source": [
        "Reading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIVgv_KZb4_h",
        "outputId": "2b3eacc0-a0db-4252-ab48-baa906aafbed"
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "\n",
        "train.head(10), test.head(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   id keyword  ...                                               text target\n",
              " 0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              " 1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              " 2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              " 3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              " 4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              " 5   8     NaN  ...  #RockyFire Update => California Hwy. 20 closed...      1\n",
              " 6  10     NaN  ...  #flood #disaster Heavy rain causes flash flood...      1\n",
              " 7  13     NaN  ...  I'm on top of the hill and I can see a fire in...      1\n",
              " 8  14     NaN  ...  There's an emergency evacuation happening now ...      1\n",
              " 9  15     NaN  ...  I'm afraid that the tornado is coming to our a...      1\n",
              " \n",
              " [10 rows x 5 columns],\n",
              "    id keyword location                                               text\n",
              " 0   0     NaN      NaN                 Just happened a terrible car crash\n",
              " 1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              " 2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              " 3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              " 4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
              " 5  12     NaN      NaN                 We're shaking...It's an earthquake\n",
              " 6  21     NaN      NaN  They'd probably still show more life than Arse...\n",
              " 7  22     NaN      NaN                                  Hey! How are you?\n",
              " 8  27     NaN      NaN                                   What a nice hat?\n",
              " 9  29     NaN      NaN                                          Fuck off!)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Tb9-pXhccgN",
        "outputId": "79803f95-7583-45e7-8add-79cc3fea4205"
      },
      "source": [
        "print(train.isnull().sum(), train.shape, test.isnull().sum(), test.shape, sep='\\n\\n')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id             0\n",
            "keyword       61\n",
            "location    2533\n",
            "text           0\n",
            "target         0\n",
            "dtype: int64\n",
            "\n",
            "(7613, 5)\n",
            "\n",
            "id             0\n",
            "keyword       26\n",
            "location    1105\n",
            "text           0\n",
            "dtype: int64\n",
            "\n",
            "(3263, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "4PYeU6q_BGPV",
        "outputId": "d80b5576-0cbf-433e-c09d-73239acc15cd"
      },
      "source": [
        "sns.countplot(x='target', data=train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9946749150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pgm3Qe1fWy3"
      },
      "source": [
        "Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rya5KSY-ndIw"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "patterns = {\n",
        "    'url'   : re.compile(r'https?://\\S+|www\\.\\S+'),\n",
        "    'html'  : re.compile(r'<.*?>'),\n",
        "    'emoji' : re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE),\n",
        "}\n",
        "\n",
        "def remove_html(text):\n",
        "    return patterns['html'].sub(r' ', text)\n",
        "\n",
        "def delete_digits(text):\n",
        "    return re.sub(r'\\d+', ' ', text)\n",
        "\n",
        "def delete_short_words(text, n=2):\n",
        "    return ' '.join([word for word in text.split() if len(word) > n])\n",
        "\n",
        "def delete_stop_words(text):\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "def special_characters(tweet): \n",
        "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
        "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)  \n",
        "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
        "    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
        "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
        "    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n",
        "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"didn`t\", \"did not\", tweet)\n",
        "    tweet = re.sub(r\"i\\x89Ûªm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"i\\x89Ûªd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"i\\x89Ûªve\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"let\\x89Ûªs\", \"let us\", tweet)\n",
        "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
        "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"that\\x89Ûªs\", \"that is\", tweet)\n",
        "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"here\\x89Ûªs\", \"here is\", tweet)\n",
        "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªre\", \"you are\", tweet)\n",
        "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"You\\x89Ûªve\", \"You have\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"You\\x89Ûªll\", \"You will\", tweet)\n",
        "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
        "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÈ\", \"\", tweet)  \n",
        "    tweet = re.sub(r\"Ì©\", \"\", tweet)\n",
        "    \n",
        "    # Character entity references\n",
        "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
        "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
        "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
        "    return tweet\n",
        "\n",
        "# Removes non-ASCII characters\n",
        "def remove_nonASCII(tweet):\n",
        "    tweet = ''.join([x for x in tweet if x in string.printable])\n",
        "    return tweet\n",
        "\n",
        "def expand_contractions(tweet):\n",
        "    \n",
        "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"i'M\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
        "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"you'd\", \"you would\", tweet)\n",
        "    tweet = re.sub(r\"You'd\", \"You would\", tweet)\n",
        "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"You've\", \"You have\", tweet)\n",
        "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"You'll\", \"You will\", tweet)  \n",
        "    tweet = re.sub(r\"y'know\", \"you know\", tweet)  \n",
        "    tweet = re.sub(r\"Y'know\", \"You know\", tweet)  \n",
        "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
        "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
        "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
        "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
        "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
        "    tweet = re.sub(r\"We've\", \"We have\", tweet) \n",
        "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
        "    tweet = re.sub(r\"We'd\", \"We would\", tweet)\n",
        "    tweet = re.sub(r\"WE'VE\", \"We have\", tweet)\n",
        "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
        "    tweet = re.sub(r\"We'll\", \"We will\", tweet)\n",
        "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
        "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
        "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
        "    tweet = re.sub(r\"They'd\", \"They would\", tweet)  \n",
        "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
        "    tweet = re.sub(r\"They've\", \"They have\", tweet)\n",
        "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
        "    tweet = re.sub(r\"They'll\", \"They will\", tweet)\n",
        "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
        "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
        "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
        "    tweet = re.sub(r\"He'll\", \"He will\", tweet)\n",
        "    tweet = re.sub(r\"she's\", \"she is\", tweet)\n",
        "    tweet = re.sub(r\"She's\", \"She is\", tweet)\n",
        "    tweet = re.sub(r\"she'll\", \"she will\", tweet)\n",
        "    tweet = re.sub(r\"She'll\", \"She will\", tweet)\n",
        "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
        "    tweet = re.sub(r\"It'll\", \"It will\", tweet)\n",
        "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"Isn't\", \"Is not\", tweet)\n",
        "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
        "    tweet = re.sub(r\"Who's\", \"Who is\", tweet)\n",
        "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
        "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
        "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
        "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"here's\", \"here is\", tweet)\n",
        "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
        "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
        "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
        "    tweet = re.sub(r\"Where's\", \"Where is\", tweet)  \n",
        "    tweet = re.sub(r\"wHeRE's\", \"where is\", tweet)  \n",
        "    tweet = re.sub(r\"how's\", \"how is\", tweet)  \n",
        "    tweet = re.sub(r\"How's\", \"How is\", tweet)  \n",
        "    tweet = re.sub(r\"how're\", \"how are\", tweet)  \n",
        "    tweet = re.sub(r\"How're\", \"How are\", tweet) \n",
        "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
        "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
        "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
        "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
        "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
        "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
        "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
        "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
        "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
        "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
        "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
        "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
        "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"DON'T\", \"Do not\", tweet)\n",
        "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
        "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
        "    tweet = re.sub(r\"DIDN'T\", \"Did not\", tweet)\n",
        "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
        "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
        "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
        "    \n",
        "    return tweet\n",
        "\n",
        "def specific_corrections(tweet):\n",
        "    \n",
        "    '''Typos, slang and informal abbreviations'''\n",
        "    \n",
        "    tweet = re.sub(r\"b/c\", \"because\", tweet)\n",
        "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
        "    tweet = re.sub(r\"w/out\", \"without\", tweet)\n",
        "    tweet = re.sub(r\"w/o\", \"without\", tweet)\n",
        "    tweet = re.sub(r\"w/\", \"with \", tweet)   \n",
        "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
        "    tweet = re.sub(r\"c/o\", \"care of\", tweet)\n",
        "    tweet = re.sub(r\"p/u\", \"pick up\", tweet)\n",
        "    tweet = re.sub(r\"\\n\", \" \", tweet)\n",
        "   \n",
        "    # Typos\n",
        "    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n",
        "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
        "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
        "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
        "    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n",
        "    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n",
        "    tweet = re.sub(r\"Newss\", \"News\", tweet)\n",
        "    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n",
        "    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n",
        "    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n",
        "    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n",
        "    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n",
        "    tweet = re.sub(r\"evng\", \"evening\", tweet)\n",
        "    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n",
        "    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n",
        "    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n",
        "    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n",
        "    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n",
        "    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n",
        "    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n",
        "    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n",
        "    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n",
        "    tweet = re.sub(r\"wrld\", \"world\", tweet)   \n",
        "    tweet = re.sub(r\"shld\", \"should\", tweet)    \n",
        "    tweet = re.sub(r\"thruuu\", \"through\", tweet)\n",
        "    tweet = re.sub(r\"probaly\", \"probably\", tweet)\n",
        "    tweet = re.sub(r\"whatevs\", \"whatever\", tweet)\n",
        "    tweet = re.sub(r\"colomr\", \"colour\", tweet)\n",
        "    tweet = re.sub(r\"pileq\", \"pile\", tweet)\n",
        "    tweet = re.sub(r\"firefightr\", \"firefighter\", tweet)\n",
        "    tweet = re.sub(r\"LAIGHIGN\", \"laughing\", tweet)\n",
        "    tweet = re.sub(r\"EXCLUSIV\", \"Exclusive\", tweet) \n",
        "    tweet = re.sub(r\"belo-ooow\", \"below\", tweet)  \n",
        "    tweet = re.sub(r\"who-ooo-ole\", \"whole\", tweet)  \n",
        "    tweet = re.sub(r\"brother-n-law\", \"father-in-law\", tweet)  \n",
        "    tweet = re.sub(r\"referencereference\", \"reference\", tweet)\n",
        "    \n",
        "    # Hashtags and usernames\n",
        "    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n",
        "    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n",
        "    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n",
        "    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
        "    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n",
        "    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n",
        "    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n",
        "    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n",
        "    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n",
        "    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n",
        "    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n",
        "    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n",
        "    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n",
        "    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n",
        "    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n",
        "    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n",
        "    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n",
        "    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n",
        "    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n",
        "    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n",
        "    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n",
        "    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n",
        "    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n",
        "    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n",
        "    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n",
        "    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n",
        "    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n",
        "    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n",
        "    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n",
        "    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n",
        "    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n",
        "    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n",
        "    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n",
        "    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n",
        "    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n",
        "    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n",
        "    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n",
        "    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n",
        "    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n",
        "    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n",
        "    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n",
        "    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n",
        "    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n",
        "    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n",
        "    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n",
        "    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park DPS\", tweet)\n",
        "    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n",
        "    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n",
        "    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n",
        "    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n",
        "    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n",
        "    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n",
        "    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n",
        "    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n",
        "    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n",
        "    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n",
        "    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n",
        "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
        "    tweet = re.sub(r\"US govt\", \"USA government\", tweet)  \n",
        "    tweet = re.sub(r\"WAwildfire\", \"WA Wildfire\", tweet)\n",
        "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
        "    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n",
        "    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n",
        "    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n",
        "    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n",
        "    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n",
        "    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n",
        "    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n",
        "    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n",
        "    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n",
        "    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n",
        "    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n",
        "    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n",
        "    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n",
        "    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n",
        "    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n",
        "    tweet = re.sub(r\"idkidk\", \"idk idk\", tweet)\n",
        "    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n",
        "    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n",
        "    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n",
        "    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n",
        "    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n",
        "    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n",
        "    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n",
        "    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n",
        "    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n",
        "    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n",
        "    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n",
        "    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n",
        "    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n",
        "    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n",
        "    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n",
        "    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n",
        "    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n",
        "    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n",
        "    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n",
        "    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n",
        "    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n",
        "    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n",
        "    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n",
        "    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n",
        "    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n",
        "    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n",
        "    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n",
        "    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n",
        "    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n",
        "    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n",
        "    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n",
        "    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n",
        "    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n",
        "    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n",
        "    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n",
        "    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n",
        "    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n",
        "    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n",
        "    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n",
        "    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n",
        "    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n",
        "    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n",
        "    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n",
        "    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n",
        "    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n",
        "    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n",
        "    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n",
        "    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n",
        "    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n",
        "    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n",
        "    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n",
        "    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n",
        "    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n",
        "    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n",
        "    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n",
        "    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n",
        "    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n",
        "    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n",
        "    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n",
        "    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n",
        "    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n",
        "    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n",
        "    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n",
        "    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n",
        "    \n",
        "    return tweet\n",
        "\n",
        "def clean_others(tweet):  \n",
        "    \n",
        "    tweet = re.sub(r\"2007he\", \"2007 he\", tweet)  \n",
        "    tweet = re.sub(r\"Hwy27\", \"Hwy 27\", tweet) \n",
        "    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)  \n",
        "    tweet = re.sub(r\"40%money\", \"40% money\", tweet)  \n",
        "    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n",
        "    tweet = re.sub(r\"wannabe\", \"wanna be\", tweet) \n",
        "    tweet = re.sub(r\"dadwho\", \"dad who\", tweet)  \n",
        "    tweet = re.sub(r\"fundwhen\", \"fund when\", tweet)\n",
        "    tweet = re.sub(r\"next chp\", \"next chapter\", tweet)\n",
        "    tweet = re.sub(r\"UR sons\", \"your sons\", tweet)  \n",
        "    tweet = re.sub(r\"Yr voice ws\", \"Your voice was\", tweet) \n",
        "    tweet = re.sub(r\"U're not\", \"You are not\", tweet)  \n",
        "    tweet = re.sub(r\"u'd win\", \"you had win\", tweet)  \n",
        "    tweet = re.sub(r\"Jus Kame\", \"Just came\", tweet)  \n",
        "    tweet = re.sub(r\"b4federal\", \"B-4, Federal\", tweet) \n",
        "    tweet = re.sub(r\"ppor child\", \"poor child\", tweet)  \n",
        "    tweet = re.sub(r\"stand ogt\", \"stand out\", tweet)\n",
        "    tweet = re.sub(r\"stand oup\", \"stand out\", tweet) \n",
        "    tweet = re.sub(r\"IS claims\", \"ISIS claims\", tweet)\n",
        "    tweet = re.sub(r\"2slow2report\", \"too slow to report\", tweet)\n",
        "    tweet = re.sub(r\"@ft\", \"@Financial Times\", tweet)\n",
        "    tweet = re.sub(r\"50ft\", \"50 ft\", tweet)\n",
        "    tweet = re.sub(r\"Ft ABH Shadow\", \"featuring ABH Shadow\", tweet)\n",
        "    tweet = re.sub(r\"Since1970the\", \"Since 1970 the\", tweet) \n",
        "    tweet = re.sub(r\"whats cracking cuz\", \"what is cracking cause\", tweet) \n",
        "    tweet = re.sub(r\"mentally ill\", \"mental illness\", tweet)\n",
        "    tweet = re.sub(r\"RIPRIPRIP\", \"RIP RIP RIP\", tweet)\n",
        "    tweet = re.sub(r\"RIPROSS\", \"RIP ROSS\", tweet)  \n",
        "    tweet = re.sub(r\"ABQ NM\", \"Albuquerque New Mexico\", tweet)\n",
        "    tweet = re.sub(r\"#BC\", \"#British Columbia\", tweet)\n",
        "    tweet = re.sub(r\"in BC\", \"in British Columbia\", tweet)\n",
        "    tweet = re.sub(r\"BC DROUGHT\", \"British Columbia Drought\", tweet)\n",
        "    tweet = re.sub(r\"in OK\", \"in Oklahoma\", tweet)\n",
        "    tweet = re.sub(r\"City OK\", \"City Oklahoma\", tweet)\n",
        "    tweet = re.sub(r\"Hinton OK\", \"Hinton Oklahoma\", tweet)\n",
        "    tweet = re.sub(r\"Guthrie OK\", \"Guthrie Oklahoma\", tweet)\n",
        "    tweet = re.sub(r\"Choctaw OK\", \"Choctaw Oklahoma\", tweet)\n",
        "    tweet = re.sub(r\"Oklahoma-OK\", \"Oklahoma City\", tweet)\n",
        "    tweet = re.sub(r\"Oklahoma [OK]\", \"Oklahoma City\", tweet)\n",
        "    tweet = re.sub(r\"JADE FL\", \"JADE Florida\", tweet) \n",
        "    tweet = re.sub(r\"Jacksonville FL\", \"Jacksonville Florida\", tweet)\n",
        "    tweet = re.sub(r\"Saint Petersburg FL\", \"Saint Petersburg Florida\", tweet)\n",
        "    tweet = re.sub(r\"Wahpeton ND\", \"Wahpeton, North Dakota\", tweet)\n",
        "    tweet = re.sub(r\"Northern Marians\", \"Northern Mariana Islands\", tweet)\n",
        "    tweet = re.sub(r\"Northern Ma\", \"Northern Mariana Islands\", tweet)\n",
        "    \n",
        "    # Abbreviation point\n",
        "    tweet = re.sub(r\"Dr\\.\", \"Doctor\", tweet)\n",
        "    tweet = re.sub(r\"f\\. M\\.O\\.P\\.\", \"featuring Mash Out Posse\", tweet)\n",
        "    tweet = re.sub(r\"M\\.O\\.P\\.\", \"Mash Out Posse\", tweet)\n",
        "    tweet = re.sub(r\"M\\.O\\.P\", \"Mash Out Posse\", tweet)\n",
        "    tweet = re.sub(r\"P\\.O\\.P\\.E\\.\", \"Pope\", tweet)\n",
        "    tweet = re.sub(r\"S\\.O\\.S\\.\", \"SOS\", tweet)\n",
        "    tweet = re.sub(r\"s\\.o\\.s\\.\", \"SOS\", tweet)  \n",
        "    tweet = re.sub(r\"Fire Co\\.\", \"Fire Company\", tweet)\n",
        "    tweet = re.sub(r\"Holt and Co\\.\", \"Holt and Company\", tweet)\n",
        "    tweet = re.sub(r\"roofing co\\.\", \"roofing company\", tweet)\n",
        "    tweet = re.sub(r\"Costa Co\\.\", \"Costa County\", tweet)\n",
        "    tweet = re.sub(r\"York Co\\.\", \"York County\", tweet)\n",
        "    tweet = re.sub(r\"Fairfax Co\\.\", \"Fairfax County\", tweet)\n",
        "    tweet = re.sub(r\"I\\.S\\.I\\.S\\.\", \"ISIS\", tweet)\n",
        "    tweet = re.sub(r\"U\\.N\\.\", \"United Nations\", tweet)\n",
        "    tweet = re.sub(r\"U\\.S\\.\", \"United States\", tweet)\n",
        "    tweet = re.sub(r\"U\\.S\", \"United States\", tweet)\n",
        "    tweet = re.sub(r\"U\\.s\\.\", \"United States\", tweet)\n",
        "    tweet = re.sub(r\"U\\.s\", \"United States\", tweet)\n",
        "    tweet = re.sub(r\"U-S\\.\", \"United States\", tweet)\n",
        "    tweet = re.sub(r\"U\\.S National\", \"United States National\", tweet)\n",
        "    tweet = re.sub(r\"LANCASTER N\\.H\\.\", \"Lancaster New Hampshire\", tweet)\n",
        "    tweet = re.sub(r\"Manchester N\\.H\\.\", \"Manchester New Hampshire\", tweet)\n",
        "   \n",
        "    # Normalization\n",
        "    tweet = re.sub(r\"\\:33333\", \"smile\", tweet)    # :33333\n",
        "    tweet = re.sub(r\"\\:\\)\\)\\)\\)\", \"smile\", tweet) # :))))\n",
        "    tweet = re.sub(r\"\\:\\)\\)\\)\", \"smile\", tweet) # :)))\n",
        "    tweet = re.sub(r\"\\:\\)\\)\", \"smile\", tweet)   # :))\n",
        "    tweet = re.sub(r\"\\:-\\)\",  \"smile\", tweet)   # :-)\n",
        "    tweet = re.sub(r\"\\;-\\)\",  \"smile\", tweet)   # ;-)\n",
        "    tweet = re.sub(r\"3\\-D\", \"smile\", tweet)  # 3-D\n",
        "    tweet = re.sub(r\"\\:O\", \"smile\", tweet)   # :O\n",
        "    tweet = re.sub(r\"\\:D\", \"smile\", tweet)   # :D\n",
        "    tweet = re.sub(r\"\\:P\", \"smile\", tweet)   # :P\n",
        "    tweet = re.sub(r\"\\:p\", \"smile\", tweet)   # :p\n",
        "    tweet = re.sub(r\"\\;\\)\", \"smile\", tweet)  # ;)\n",
        "    tweet = re.sub(r\"\\:\\)\", \"smile\", tweet)  # :)\n",
        "    tweet = re.sub(r\"\\=\\)\", \"smile\", tweet)  # =)\n",
        "    tweet = re.sub(r\"\\^\\^\", \"smile\", tweet)  # ^^\n",
        "    tweet = re.sub(r\"\\:-\\(\", \"sad\", tweet)   # :-(\n",
        "    tweet = re.sub(r\"\\:\\(\", \"sad\", tweet)    # :(\n",
        "    tweet = re.sub(r\"\\=\\(\", \"sad\", tweet)    # =(\n",
        "    tweet = re.sub(r\"\\-\\_\\_\\-\", \"\", tweet)   # -__-\n",
        "    tweet = re.sub(r\"\\.\\_\\.\", \"\", tweet)     # ._.\n",
        "    tweet = re.sub(r\"T\\_T\", \"\", tweet)       # T_T\n",
        "    \n",
        "    return tweet\n",
        "\n",
        "# Remove unwanted words\n",
        "def remove_non_alnum(tweet):\n",
        "    punctuation = re.compile('[^A-Za-z0-9]+')\n",
        "    return punctuation.sub(r' ',tweet)\n",
        "\n",
        "# Remove punctuations.\n",
        "def remove_punct(text):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "# Remove leading, trailing, and extra spaces\n",
        "def remove_extra_spaces(text):\n",
        "    text = re.sub('\\s+', ' ', text).strip() \n",
        "    return text\n",
        "\n",
        "# Normalization\n",
        "abbreviations = {\n",
        "    \n",
        "    \"i.e\":\"that is\", \"mofo\":\"mother fucker\", \"til\":\"till\",\n",
        "    \"ft.\":\"featuring\", \"mf\":\"mother fucker\", \"bout\":\"about\",\n",
        "    \"ft\":\"featuring\", \"mfs\":\"mother fucker\", \"nd\":\"and\", \n",
        "    \"feat.\":\"featuring\", \"ltd\":\"limited\", \"nvr\":\"never\",\n",
        "    \"feat\":\"featuring\", \"pls\":\"please\", \"ppl\":\"people\",\n",
        "    \"tbs\":\"tablespoons\", \"tho\":\"though\", \"fav\":\"favorite\",\n",
        "    \"bc\":\"because\", \"cuz\":\"because\", \"bcuz\":\"because\",\n",
        "    \"btwn\":\"between\", \"fwy\":\"Freeway\", \"hwy\":\"Highway\",\n",
        "    \"diff\":\"different\", \"appx\":\"approximately\", \n",
        "    \"im\":\"I am\", \"ive\":\"I have\", \"uve\":\"you have\", \n",
        "    \"youd\":\"you had\", \"hadnt\":\"had not\", \"isnt\":\"is not\",\n",
        "    \"dont\":\"do not\", \"didnt\":\"did not\", \"cant\":\"cannot\",\n",
        "    \"urself\":\"yourself\", \"wont\":\"would not\", \n",
        "    \"heres\":\"Here is\", \"lets\":\"Let us\", \"2day\":\"today\", \n",
        "    \"s2g\":\"swear to god\", \"be4\":\"before\", \"b4\":\"before\", \n",
        "    \"4the\":\"for the\", \"1st\":\"first\",\n",
        "   \n",
        "    # location\n",
        "    \"okwx\":\"Oklahoma Weather\", \"arwx\":\"Arkansas Weather\",    \n",
        "    \"gawx\":\"Georgia Weather\", \"cawx\":\"California Weather\",\n",
        "    \"tnwx\":\"Tennessee Weather\", \"azwx\":\"Arizona Weather\",  \n",
        "    \"alwx\":\"Alabama Weather\", \"scwx\":\"South Carolina Weather\",\n",
        "    \"isis\":\"Islamic State\", \"okc\":\"Oklahoma\",\"oun\":\"Oklahoma\",\n",
        "    \"isil\":\"Islamic State\", \"suruc\":\"Urfa\", \"pdx\":\"Portland\", \n",
        "    \"nm\":\"New Mexico\", \"newyork\":\"New York\", \"alska\":\"Alaska\",\n",
        "    \"nh\":\"New Hampshire\", \"nyc\":\"New York City\",\n",
        "    \"cnmi\":\"Northern Mariana Islands\", \"calif\":\"California\",\n",
        "    \"sarabia\":\"Saudi Arabia\", \"saudiarabia\":\"Saudi Arabia\", \n",
        "    \"mh370\":\"Malaysia Airlines Flight 370\", \n",
        "    \n",
        "    # units\n",
        "    \"12hr\":\"12 hr\",\"16yr\":\"16 year\", \"hrs\":\"hour\",\"hr\":\"hour\",\n",
        "    \"19yrs\":\"19 year\", \"yrs\":\"year\", \"min\":\"minute\", \n",
        "    \"20yrs\":\"20 year\", \"yr\":\"year\", \"mins\":\"minute\", \n",
        "    \n",
        "    # Typos\n",
        "    \"tren\":\"trend\", \"kno\":\"know\", \"swea\":\"swear\", \"stil\":\"still\",\n",
        "    \"fab\":\"fabulous\", \"srsly\":\"seriously\", \"epicente\":\"epicenter\", \n",
        "    \"jumpin\":\"jumping\", \"burnin\":\"burning\", \"throwin\":\"throwing\",\n",
        "    \"killin\":\"killing\", \"nothin\":\"nothing\", \"thinkin\":\"thinking\",  \n",
        "    \"tryin\":\"trying\", \"lookg\":\"looking\", \"fforecast\":\"Forecast\",\n",
        "    \"comin\":\"Coming\", \"newss\":\"news\", \"memez\":\"meme\", \"oli\":\"oil\",\n",
        "}\n",
        "\n",
        "def convert_abbrev(word):\n",
        "    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n",
        "\n",
        "def convert_abbrev_in_text(text):\n",
        "    tokens = nltk.tokenize.word_tokenize(text)\n",
        "    tokens = [convert_abbrev(word) for word in tokens]\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    for pattern in patterns.values():\n",
        "        text = pattern.sub(r' ', text)\n",
        "    \n",
        "    text = special_characters(text)\n",
        "    text = remove_nonASCII(text)\n",
        "    text = expand_contractions(text)\n",
        "    text = specific_corrections(text)\n",
        "    text = remove_html(text)\n",
        "    text = clean_others(text)\n",
        "    text = convert_abbrev_in_text(text)\n",
        "    text = remove_punct(text)\n",
        "    text = remove_non_alnum(text)\n",
        "    text = delete_stop_words(text)\n",
        "    text = delete_digits(text)\n",
        "    text = delete_short_words(text)\n",
        "    text = remove_extra_spaces(text)\n",
        "    return text.lower().strip()\n",
        "\n",
        "train['clean_text'] = train.text.apply(clean_text)\n",
        "test['clean_text']  = test.text.apply(clean_text)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRKeGZV2SH9X"
      },
      "source": [
        "Apply lemmatization and stem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtSrqYnPST3"
      },
      "source": [
        "def lemmatize(text):\n",
        "    return ' '.join([nltk.stem.WordNetLemmatizer().lemmatize(w) for w in text.split()])\n",
        "\n",
        "def stem(text):\n",
        "    return ' '.join(nltk.stem.PorterStemmer().stem(w) for w in text.split())\n",
        "\n",
        "\n",
        "train['tokenized_text'] = train['clean_text'].apply(lemmatize).apply(stem).apply(str.strip)\n",
        "test['tokenized_text']  = test['clean_text'].apply(lemmatize).apply(stem).apply(str.strip)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFbBsA1E2d-U"
      },
      "source": [
        "TF-IDF vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ASJ0fvxfQdt",
        "outputId": "6fc7b92a-7ce0-441a-932e-e750f1483590"
      },
      "source": [
        "tfidf = TfidfVectorizer(stop_words='english', \n",
        "                        ngram_range=(1, 1))\n",
        "tfidf.fit(train['tokenized_text'])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBXrQFhUiHG6"
      },
      "source": [
        "Models for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ws_80fCiGNa"
      },
      "source": [
        "def get_model(model_name, iterations=100_000):\n",
        "    models = {\n",
        "        'bayes'   :  MultinomialNB(),\n",
        "\n",
        "        'log_reg' :  LogisticRegression(\n",
        "            max_iter = iterations,\n",
        "            solver = 'sag',\n",
        "            fit_intercept = False,\n",
        "            penalty = 'l2',\n",
        "            dual = False,\n",
        "            verbose = 0),\n",
        "\n",
        "        'svc' : SVC(\n",
        "            C=2,\n",
        "            tol=1e-4,\n",
        "        )\n",
        "    }\n",
        "\n",
        "    return models[model_name]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8UuVNgTsKfp",
        "outputId": "f5916d9c-c2a5-480a-8d15-a87a0bc663c3"
      },
      "source": [
        "def fold(X, tfidf_vectorizer, model_name='bayes', iterations=100_000, k=5):\n",
        "    y = X.target\n",
        "    aucs, f1s, models = [], [], []\n",
        "    for train_idx, test_idx in KFold(k, shuffle=True).split(X):\n",
        "        X_train, X__test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y__test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        model = get_model(model_name)\n",
        "\n",
        "        train_vectors = tfidf_vectorizer.transform(X_train['tokenized_text'])\n",
        "        test__vectors = tfidf_vectorizer.transform(X__test['tokenized_text'])\n",
        "\n",
        "        model.fit(train_vectors, y_train)\n",
        "\n",
        "        y_score = model.predict(test__vectors)\n",
        "\n",
        "        auc = roc_auc_score(y__test, y_score)\n",
        "        f1  = f1_score(y__test, y_score)\n",
        "\n",
        "        aucs.append(auc)\n",
        "        f1s.append(f1)\n",
        "        models.append(model)\n",
        "        print(\"\"\"\n",
        "ROC AUC: {}\n",
        "F1     : {}\n",
        "\"\"\".format(auc, f1))\n",
        "        \n",
        "    print(\"\"\"\n",
        "\n",
        "\n",
        "Mean ROC AUC: {}\n",
        "Mean F1     : {}\n",
        "\"\"\".format(np.mean(aucs), np.mean(f1s)))\n",
        "    \n",
        "    return models\n",
        "\n",
        "\n",
        "models = fold(train, tfidf, model_name='svc', k=8, iterations=500_000)    "
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ROC AUC: 0.77553059864852\n",
            "F1     : 0.7280334728033473\n",
            "\n",
            "\n",
            "ROC AUC: 0.7967135476463835\n",
            "F1     : 0.7608982826948482\n",
            "\n",
            "\n",
            "ROC AUC: 0.7765007389704726\n",
            "F1     : 0.7319727891156463\n",
            "\n",
            "\n",
            "ROC AUC: 0.8019927536231883\n",
            "F1     : 0.7639836289222374\n",
            "\n",
            "\n",
            "ROC AUC: 0.7537364553493586\n",
            "F1     : 0.7061923583662714\n",
            "\n",
            "\n",
            "ROC AUC: 0.7722249408694235\n",
            "F1     : 0.726027397260274\n",
            "\n",
            "\n",
            "ROC AUC: 0.7893024815560027\n",
            "F1     : 0.7506631299734748\n",
            "\n",
            "\n",
            "ROC AUC: 0.7973476999585578\n",
            "F1     : 0.7525325615050652\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Mean ROC AUC: 0.7829186520777384\n",
            "Mean F1     : 0.7400379525801456\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZatTz2vZcTl"
      },
      "source": [
        "TF-IDF transformation for train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM9EOyBiYxYh"
      },
      "source": [
        "test__vectors = tfidf.transform(test ['tokenized_text'])\n",
        "train_vectors = tfidf.transform(train['tokenized_text'])"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v48tLCWUxTlY"
      },
      "source": [
        "Kaggle submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hij18gzjxVDF"
      },
      "source": [
        "model = get_model('svc', iterations=500_000)\n",
        "model.fit(train_vectors, train.target)\n",
        "y_scores = model.predict(test__vectors)"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMFffqiLx-Qr",
        "outputId": "766ea0f5-b2b7-486f-8736-5bbe7b9d9c92"
      },
      "source": [
        "kaggle_frame = pd.DataFrame({ 'id' : test.id, 'target' : y_scores })\n",
        "kaggle_frame.target.value_counts()"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2094\n",
              "1    1169\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9shjDZrvy2WG"
      },
      "source": [
        "kaggle_frame.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 157,
      "outputs": []
    }
  ]
}